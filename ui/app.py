import streamlit as st
import json
import streamlit.components.v1 as components
from utils import load_thread_data, get_thread_by_id

st.set_page_config(page_title="ThreadNavigatorAI 2.0", layout="wide")
st.title("🧵 ThreadNavigatorAI 2.0")

# 🧬 How it Works
with st.expander("🧬 How This Works (click to expand)", expanded=False):
    st.markdown("""
    **ThreadNavigatorAI 2.0** is a modular, multi-agent pipeline that mimics how a human analyst might navigate noisy Reddit threads:

    1. 🔍 **Summarizer Agent** condenses user-generated content using semantic transformers.
    2. 🧪 **Fact Checker Agent** flags or affirms claims using external tool-augmented retrieval.
    3. 🧠 **Evaluator Agent** applies LLM-based rubrics (Relevance, Factuality, Coherence).

    The system supports both real API calls and mock simulations to benchmark scalability under OpenRouter’s free-tier.
    """)

# 🧭 Onboarding (one-time info message)
if "show_guide" not in st.session_state:
    st.session_state.show_guide = True
if st.session_state.show_guide:
    st.info("👋 Welcome! Use the selector below to explore 100 Reddit threads processed by AI agents. You can toggle latency, evaluation, and download options in the sidebar.")
    if st.button("Got it! Hide this message"):
        st.session_state.show_guide = False

# Sidebar: About + Toggle Controls
st.sidebar.markdown("## 📚 About")
st.sidebar.markdown("ThreadNavigatorAI analyzes Reddit threads using multi-agent LLMs.")
with st.sidebar.expander("⚙️ Toggle Display Options"):
    show_latency = st.checkbox("Show Latency", value=True)
    show_eval = st.checkbox("Show Evaluation", value=True)
    show_download = st.checkbox("Enable Download", value=True)
st.sidebar.markdown("---")
st.sidebar.markdown("A multi-agent Reddit thread analyzer built by Rajesh 💼")

# Load data
data = load_thread_data()
thread_ids = [t["thread_id"] for t in data]

# Thread selector
selected_id = st.selectbox("🎯 Select a Reddit Thread", thread_ids)
thread = get_thread_by_id(data, selected_id)

# Reset copy flag when thread changes
if "last_selected_id" not in st.session_state:
    st.session_state.last_selected_id = selected_id
if selected_id != st.session_state.last_selected_id:
    st.session_state.summary_copied = False
    st.session_state.last_selected_id = selected_id

if thread:
    st.subheader("📌 Title:")
    st.markdown(f"**{thread['title']}**")

    # Summary type and source
    is_mock = thread.get("is_mock", False)
    summary_type = "🤖 Simulated Summary" if is_mock else "🔁 Real Summary"
    summary_color = "#e0e0e0" if is_mock else "#d4edda"
    source_url = thread.get("source_url", "")
    source_html = f'<b>📎 Source:</b> <a href="{source_url}" target="_blank">Reddit Thread</a>' if source_url else ""

    st.markdown(f"""
        <div style='background-color:{summary_color};padding:10px;border-radius:10px;margin-bottom:10px'>
            <b>Summary Type:</b> {summary_type}<br>
            {source_html}
        </div>
    """, unsafe_allow_html=True)

    with st.expander("💬 View Original Posts", expanded=False):
        for post in thread["posts"]:
            st.markdown(f"- {post}")


    # Define tab labels dynamically
    tab_labels = ["🧠 Summary", "🔎 Fact Checks"]
    if show_latency:
        tab_labels.append("⚡ Latency")
    if show_eval:
        tab_labels.append("📊 Evaluation")

    tabs = st.tabs(tab_labels)
    tab_index = 0

    # 🧠 Summary Tab
    with tabs[tab_index]:
        st.markdown("### Summary")
        st.markdown(
            f"""
            <div style='background-color:#f8f9fa; padding:12px; border-radius:8px;
                        font-family:monospace;  word-wrap:break-word;
                        border:1px solid #ddd; margin-bottom:10px'>
                {thread["summary"]}
            </div>
            """, unsafe_allow_html=True)

        # Model attribution
        model_summary = thread.get("models_used", {}).get("summarizer", "Unknown")
        st.markdown(f"<span style='color:gray;font-size:small'>🧠 Generated by: `{model_summary}`</span>", unsafe_allow_html=True)

        if show_download:
            json_str = json.dumps(thread, indent=2)
            st.download_button(
                "⬇️ Download JSON Result",
                data=json_str,
                file_name=f"{thread['thread_id']}.json",
                mime="application/json"
            )

    tab_index += 1

    # 🔎 Fact Checks Tab
    with tabs[tab_index]:
        st.markdown("### Fact Checks")
        model_fc = thread.get("models_used", {}).get("factchecker", "Unknown")
        st.markdown(f"<span style='color:gray;font-size:small'>🔎 Checked by: `{model_fc}`</span>", unsafe_allow_html=True)
        for f in thread["fact_check"]:
            judgment = f["judgment"]
            emoji = "🟢" if "Correct" in judgment else "🔴" if "Incorrect" in judgment else "⚪"
            st.markdown(f"- **Claim:** {f['claim']}  \n  **Judgment:** {emoji} {judgment}")

    tab_index += 1

    # ⚡ Latency Tab
    if show_latency:
        with tabs[tab_index]:
            st.markdown("### Agent Latency")
            latency = thread.get("latency", {})
            models = thread.get("models_used", {})
            if latency:
                for k, v in latency.items():
                    model_name = models.get(k.lower(), "Unknown")
                    st.markdown(f"- **{k.capitalize()}**: {v} sec  \n  <span style='color:gray;font-size:small'>Model: `{model_name}`</span>", unsafe_allow_html=True)
            else:
                st.info("Latency not recorded for this thread.")
        tab_index += 1

    # 📊 Evaluation Tab
    if show_eval:
        with tabs[tab_index]:
            st.markdown("### Evaluation")
            eval_ = thread["evaluation"]
            if isinstance(eval_, str):
                eval_ = json.loads(eval_)
            if isinstance(eval_, dict):
                for k, v in eval_.items():
                    score = v["score"]
                    reason = v["reason"]
                    emoji = "🟢" if score >= 4 else "🟡" if score == 3 else "🔴"
                    st.markdown(f"- **{k.capitalize()}**: {emoji} {score} — {reason}")
            else:
                st.warning(f"Evaluation skipped: {eval_}")

    st.caption("✅ Powered by OpenRouter LLMs and modular multi-agent stack.")